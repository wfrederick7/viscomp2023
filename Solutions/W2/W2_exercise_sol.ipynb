{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quickstart COLAB\n",
    "from pathlib import Path\n",
    "if Path.cwd().name != 'W2':\n",
    "  !git clone --quiet https://github.com/tavisualcomputing/viscomp2023/\n",
    "  %cd viscomp2023/Solutions/W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Background subtraction\n",
    "\n",
    "This python notebook includes all the exercises from this session. All the needed setup, I/O and user-interaction code is already provided to let you concentrate on this week's topic.<br>\n",
    "Please run all the provided code in order before running yours.<br>\n",
    "\n",
    "This week's assignment:\n",
    "\n",
    "## 1. Bluescreen\n",
    "### 1.A Single pixel model\n",
    "Extract foreground objects in bluescreen movie footage. The blue background color should be specified by a single color value [r, g, b]. You may obtain such a value by using the provided notebook widget to select one or more blue pixels on the first frame of the footage. To determine whether a given pixel should be foreground or background, threshold the absolute distance of its value to the reference value, i.e.\n",
    "\n",
    "$$\\left\\lVert[r, g, b] âˆ’ [r_0, g_0, b_0]\\right\\rVert < t$$\n",
    "\n",
    "Based on pixelwise decisions, a mask can be created to specify foreground and background.<br>\n",
    "Try different values for the threshold and the reference color. To test your results, you can use the short video provided. Note that not the whole background consists of bluescreen.\n",
    "To overcome this problem, you can use the precomputed mask provided into the variable `mask`.\n",
    "\n",
    "### 1.B Exemplar set model\n",
    "To further improve the results, specify the blue background by an exemplar set of blue background pixels. You may select this set using the same widget as in the previous point. After computing mean $\\mu$ and covariance matrix $\\Sigma$ of the pixel values, the Mahalanobis distance $M(x, \\mu, \\Sigma)$ can be used to decide whether a color value $x$ originated from the background or the foreground.\n",
    "\n",
    "$$M(x, \\mu, \\Sigma) = \\sqrt{(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$$\n",
    "\n",
    "## 2. Per-pixel Model\n",
    "Probably you have noticed that the approach in the previous task failed in the upper region of the background, which was not covered with blue sheets, for which we provided the `mask`. To handle more complex scenarios, extend the background model to a per-pixel model. For each pixel, compute the mean and the covariance of its values over a number of frames which do not contain foreground objects. You may then use the Mahalanobis distance to classify pixels in sequences containing foreground objects. You can use the same threshold for all pixels.<br>\n",
    "To create the model, use jugglingBG.avi. This scene was captured for several frames without foreground objects. Based on the background model, foreground pixels can then be identified and masks can be created for jugglingTest.avi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful information for this tutorial:\n",
    "\n",
    "Images are represented as numpy arrays, which are fixed-dimensional arrays. Videos are represented as arrays of images.<br>\n",
    "In this tutorial each pixel takes a RGB value within the range [0-255] and is represented as a 16-bit signed integer, to allow signed operations without subtle overflow bugs.\n",
    "\n",
    "you will need to visualize your results, thus to printout an image use:\n",
    "\n",
    "`imshow(image)`\n",
    "\n",
    "You can read pixel values inside an image by accessing the corresponding pixel coordinates in the array. The following reads the green component (1) of pixel at location (10, 20) of the 5th frame (index 4):\n",
    "\n",
    "`g = vid[4, 10, 20, 1]`\n",
    "\n",
    "Of course you can also change pixel values in the same way. We recommend copying an image before editing. To copy an image:\n",
    "\n",
    "`im_copy = np.array(im)` (`np` is the short name for `numpy` which is recognized by python when importing numpy with `import numpy as np`)\n",
    "\n",
    "With numpy it is also possible to refer to slices of a given array by specifying the kept dimensions in the slice with `:`. For example, if we want to access the red channel of an RGB image `im` with shape `(height, width, 3)`, we can write `im[:, :, 0]`, which refers to the whole image data for the red channel only. We can optionally specify a range to be kept along a direction by writing the extreme indices around the `:` as in `min:max`. For example, accessing a patch within the pixel coordinates `(h0, w0)`, `(h1, w1)` with h0 < h1 and w0 < w1 can be done with `im[h0:h1, w0:w1, :]`.<br>\n",
    "You can also index arrays based on conditions: for example to set to black all pixels with a blue component higher than 200 you can write `im[im[:, :, 2] > 200] = 0`.<br>\n",
    "These references can be used both for reading an image slice and for writing a whole slice at once, without looping over it.\n",
    "\n",
    "When doing operations between numpy arrays, all operations are executed element-wise (except for the `@` operator that performs the usual matrix multiplication). This requires the shapes (dimensions) of the involved arrays to match or to be compatible according to broadcasting rules, which are usually intuitive. An example of broadcasting is when adding a pixel value `p=np.array([70, 128, 255])` to an image `im` with shape `(height, width, 3)`: the operation is legal and the pixel value is added independently to each pixel in the image. A complete reference of numpy broadcasting rules (not necessary for this tutorial, but very useful for numpy python programming) can be found in https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html . \n",
    "\n",
    "In the following we list some useful functions:\n",
    "\n",
    "`np.array(a)`: returns a copy of `a` as a numpy array. Argument `a` does not need to be a numpy array, it can be any array-like object, e.g. a python list. https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html<br>\n",
    "`np.empty(shape, dtype)`: returns a new empty array with specified shape `shape` and the specified data type `dtype`. For this tutorial we suggest using as dtype `np.int16` for images and default for the rest. https://docs.scipy.org/doc/numpy/reference/generated/numpy.empty.html<br>\n",
    "`np.ones(shape, dtype)`: returns a new array filled with ones with specified shape `shape` and the specified data type `dtype`. For this tutorial we suggest using as dtype `np.int16` for images and default for the rest. https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html<br>\n",
    "`np.zeros(shape, dtype)`: returns a new array filled with zeros with specified shape `shape` and the specified data type `dtype`. For this tutorial we suggest using as dtype `np.int16` for images and default for the rest. https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html<br>\n",
    "`np.eye(N)` : builds the identity matrix of order `N`. https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html<br>\n",
    "`np.reshape(a, newshape)`: returns a reshaped view of the input array `a` having shape `newshape`, which should be specified as a tuple of integers. https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html<br>\n",
    "`np.linalg.norm(x)`: returns the L2 norm of array `x`. You can also specify a keyword argument `axis` to compute the norm only along a specific dimension (the result will have the same shape as `x`, but without the specified dimension) https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html<br>\n",
    "`np.transpose(a)` : returns the transposed of array `a`, whose shape has reversed order with respect to `a`. Alias `a.T`. https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html<br>\n",
    "`np.linalg.inv(a)` : returns the multiplicative inverse of matrix `a`. https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html<br>\n",
    "`np.matmul(a, b)` : returns the matrix multiplication of `a` multiplied by `b`. Alias `a @ b`. https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html<br>\n",
    "`np.sum(a)` : returns the sum of all the elements of array `a`. You can specify a keyword argument `axis` to compute the sum only along a specific dimension (the result will have the same shape as `a`, but without the specified dimension) https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html<br>\n",
    "`np.mean(a)` : returns the mean value of all the elements of array `a`. You can specify a keyword argument `axis` to compute the mean only along a specific dimension (the result will have the same shape as `a`, but without the specified dimension) https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html<br>\n",
    "`np.cov(m)` : build the covariance matrix of array `a` where each row represents a variable and each column represents an observation.  https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1 - SETUP CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell installs required packages into the virtual machine that will run your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-video\n",
    "!pip install scikit-image\n",
    "!pip install ipywidgets\n",
    "!pip install matplotlib\n",
    "# !pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages to read videos, make interactive widgets and plot images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo.io\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "from ipywidgets import interactive, widgets\n",
    "from IPython.display import display\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "\n",
    "# To solve ffmpeg problems:\n",
    "\n",
    "# Mac: brew install ffmpeg\n",
    "# Linux: sudo apt-get install ffmpeg\n",
    "# Windows: https://stackoverflow.com/questions/75705876/why-is-sk-video-not-working-even-after-declaring-the-ffmpeg-path\n",
    "\n",
    "# To solve the error with np.float(): pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the test video bluescreen.avi in `bluescreen` and the upper region mask into `umask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluescreen = np.array(skvideo.io.vread(\"bluescreen.avi\"), dtype=np.int16)\n",
    "numframes, height, width, channels = np.shape(bluescreen)\n",
    "print(\"Video information:\")\n",
    "print(\"Number of frames: \", numframes)\n",
    "print(\"Frame width (pixels): \", width)\n",
    "print(\"Frame height (pixels): \", height)\n",
    "print(\"Video array shape: \", np.shape(bluescreen))\n",
    "\n",
    "umask = skimage.io.imread(\"mask.bmp\")//255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a \\*wonderful\\* user interface we have prepared for you to select any number of points on the first frame of the video.\n",
    "\n",
    "Use the x slider to move vertically and use the y slider to move horizontally (x and y reflect indexing in the image array).<br>\n",
    "Press \"Update image\" when you want to see where you moved your cursor within the image, and press \"Remember this point\" when you want to store the current selected point into the python list `selected_image_locations`.\n",
    "\n",
    "\n",
    "`selected_image_locations` will contain all selected locations stored as couples of integers, taking the format: [(x1, y1), (x2, y2), (x3, y3) ...]\n",
    "\n",
    "The image will show a red cross where there is the current temporary selection, and green crosses on all stored locations.\n",
    "\n",
    "If you are not familiar with numpy arrays, we suggest to read through the code of `display_selections` and `draw_cross` as further examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selected_image_locations = list()\n",
    "\n",
    "def draw_cross(im, cx, cy, l, w, col):\n",
    "    dimx, dimy = np.shape(im)[:2]\n",
    "    im[max(0, cx-l):min(cx+l, dimx-1), max(0, cy-w):min(dimy-1, cy+w)] = col\n",
    "    im[max(0, cx-w):min(dimx-1, cx+w), max(0, cy-l):min(dimy-1, cy+l)] = col\n",
    "\n",
    "\n",
    "def display_selections(x, y):\n",
    "    tmp_im = np.array(bluescreen[0]*umask)\n",
    "    for px, py in selected_image_locations:\n",
    "        draw_cross(tmp_im, px, py, 1, 5, (0, 255, 0))\n",
    "    draw_cross(tmp_im, x, y, 1, 5, (255, 0, 0))\n",
    "    imshow(tmp_im)\n",
    "    \n",
    "interactive_plot = interactive(display_selections,{'manual': True, 'manual_name': 'Update image'}, x=(0, height-1, 1), y=(0, width-1, 1))\n",
    "button = widgets.Button(description=\"Remember this point\")\n",
    "xslider = interactive_plot.children[0]\n",
    "yslider = interactive_plot.children[1]\n",
    "button.on_click(lambda b: selected_image_locations.append((xslider.value, yslider.value)))\n",
    "display(button)\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1.A - PUT SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few basic code hints this time, just in case it is the very first time you code in Python :)\n",
    "# of course this is not a template for the 'best' solution, you are encouraged to change everything you want!\n",
    "\n",
    "# First we take the x and y pixel coordinates of the first selection made at the previous step. \n",
    "# Notice that selected_image_location[0] is a tuple of 2 elements, which we can assign to two scalar variables at once. If the dimensions don't match (like assigning a couple to three variables), Python will throw an error\n",
    "selx, sely = selected_image_locations[0]\n",
    "\n",
    "# get the [r0, g0, b0] values that model our background in this simple single-pixel model\n",
    "ref_rgb = bluescreen[0, selx, sely]\n",
    "\n",
    "# set a threshold, you can tune it experimentally after seeing the results\n",
    "THRESHOLD = 180\n",
    "\n",
    "# initialize the background mask, it will need to be a 0-1 array of the same sizes of our video frames, so that each 0-1 value corresponds to a pixel being assigned to class background (0) or foreground (1)\n",
    "bgmask = np.array(umask) # HxW # np.ones_like(bluescreen[0, :, :, 0])\n",
    "\n",
    "# now classify each pixel according to the single-pixel model\n",
    "for h in range(height):\n",
    "    for w in range(width):\n",
    "        if np.linalg.norm(bluescreen[0, h, w] - ref_rgb) < THRESHOLD:\n",
    "            bgmask[h, w] = 0.0\n",
    "\n",
    "# alternatively you can also do it with one line of code using conditional indexing:\n",
    "bgmask[np.linalg.norm(bluescreen[0] - ref_rgb, axis=-1) < THRESHOLD] = 0\n",
    "\n",
    "# and visualize your results on the first frame. Do they look reasonable?\n",
    "imshow(bluescreen[0]*bgmask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the quality of the solution here depends on the first chosen point! You can play with selected points and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap masking in a function\n",
    "def detect_bg(i):\n",
    "    bgmask = np.array(umask)\n",
    "    bgmask[np.linalg.norm(bluescreen[i] - ref_rgb, axis=-1) < THRESHOLD] = 0\n",
    "    return bgmask\n",
    "\n",
    "play = widgets.Play(\n",
    "    interval=1000,\n",
    "    min=0,\n",
    "    max=numframes-1,\n",
    "    step=1,\n",
    ")\n",
    "\n",
    "# mask each frame\n",
    "def print_frame(i):\n",
    "    bg = detect_bg(i)\n",
    "    # We want to see the background just a bit, so we keep 25% of its brightness!\n",
    "    float_mask = bg + 0.25 * (1 - bg)\n",
    "    imshow(bluescreen[i] / 255. * float_mask)\n",
    "    \n",
    "slider = interactive(print_frame, i=(0, numframes-1, 1))\n",
    "widgets.jslink((play, 'value'), (slider.children[0], 'value'))\n",
    "widgets.VBox([play, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1.B - PUT SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this time the Mahalanobis distance for the thresholding, with a global mean and covariance of the background\n",
    "assert(len(selected_image_locations) > 1)\n",
    "print(selected_image_locations)\n",
    "exemplar_pixel_set = np.array([bluescreen[0, selx, sely] for selx, sely in selected_image_locations])  # the [`expression` for `element` in `iterator`] list constructor is just an equivalent pythonic short-hand for a for-loop\n",
    "mean = np.mean(exemplar_pixel_set, axis=0)\n",
    "cov = np.cov(exemplar_pixel_set.T)\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "THRESHOLD = 110\n",
    "\n",
    "def detect_bg_mah_loop(i):\n",
    "    \"\"\"\n",
    "    Compute the background mask using the mahalanobis distance.\n",
    "    \n",
    "    This function is perfectly equivalent to detect_bg_mah_noloop, but uses explitic python looping\n",
    "    \"\"\"\n",
    "    bgmask = np.array(umask)\n",
    "    \n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            residual = bluescreen[i, h, w]-mean\n",
    "            if residual.T @ inv_cov @ residual < THRESHOLD**2:\n",
    "                bgmask[h, w] = 0\n",
    "\n",
    "    return bgmask\n",
    "\n",
    "def detect_bg_mah_noloop(i):\n",
    "    \"\"\"\n",
    "    Compute the background mask using the mahalanobis distance.\n",
    "    \n",
    "    This function is perfectly equivalent to detect_bg_mah_loop, but uses implicit numpy broadcasting rules (gains in performance)\n",
    "    \"\"\"\n",
    "    bgmask = np.array(umask)\n",
    "    \n",
    "    residuals = bluescreen[i]-mean  # precompute all differences from the mean value\n",
    "    distance_image = np.sum((residuals[:, :, np.newaxis, :] @ inv_cov)[:, :, 0, :] * residuals, axis=-1)  # build an image where each pixel is the mahalanobis distance from the corresponding source pixel. See docpage of matmul for details on its specific broadcasting rules.\n",
    "    \n",
    "    bgmask[distance_image < THRESHOLD**2] = 0  # conditional assignment of mask values based on the precomputed distance image.\n",
    "    return bgmask\n",
    "\n",
    "# and visualize your results on the first frame. Do they look reasonable?\n",
    "imshow(bluescreen[0]*detect_bg_mah_loop(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the quality of the solution here depends on the chosen points! You can play with selected points and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = widgets.Play(\n",
    "    interval=1000,\n",
    "    min=0,\n",
    "    max=numframes-1,\n",
    "    step=1,\n",
    ")\n",
    "\n",
    "def print_frame(i):\n",
    "    bg = detect_bg_mah_loop(i)\n",
    "    # We want to see the background just a bit, so we keep 25% of its brightness!\n",
    "    float_mask = bg + 0.25 * (1 - bg)\n",
    "    imshow(bluescreen[i] / 255. * float_mask)\n",
    "    \n",
    "slider = interactive(print_frame, i=(0, numframes-1, 1))\n",
    "widgets.jslink((play, 'value'), (slider.children[0], 'value'))\n",
    "widgets.VBox([play, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2 - SETUP CODE\n",
    "(installs and imports from the Exercise 1 should be run anyway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the test video jugglingTest.avi in `testvid` and the background reference video jugglingBG.avi into `bgvid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgvid = np.array(skvideo.io.vread(\"jugglingBG.avi\"), dtype=np.int16)\n",
    "testvid = np.array(skvideo.io.vread(\"jugglingTest.avi\"), dtype=np.int16)\n",
    "\n",
    "bgnumframes, bgheight, bgwidth, bgchannels = np.shape(bgvid)\n",
    "print(\"jugglingBG.avi video information:\")\n",
    "print(\"Number of frames: \", bgnumframes)\n",
    "print(\"Frame width (pixels): \", bgwidth)\n",
    "print(\"Frame height (pixels): \", bgheight)\n",
    "print(\"Video array shape: \", np.shape(bgvid))\n",
    "\n",
    "print()\n",
    "testnumframes, testheight, testwidth, testchannels = np.shape(testvid)\n",
    "print(\"jugglingTest.avi video information:\")\n",
    "print(\"Number of frames: \", testnumframes)\n",
    "print(\"Frame width (pixels): \", testwidth)\n",
    "print(\"Frame height (pixels): \", testheight)\n",
    "print(\"Video array shape: \", np.shape(testvid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2 - PUT SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the inverse covariance matrix for every pixel is going to require some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(bgvid, axis=0) # extract the mean value for each pixel by averaging over subsequent frames\n",
    "inv_cov_matrices = np.empty((bgheight, bgwidth, 3, 3))\n",
    "\n",
    "progress_bar = widgets.IntProgress(min=0.0, max=bgwidth, description='')\n",
    "display(progress_bar)\n",
    "for w in range(bgwidth):\n",
    "    for h in range(bgheight):\n",
    "        inv_cov_matrices[h, w] = np.linalg.inv(np.cov(bgvid[:, h, w, :].T) + 1e-8*np.eye(3))\n",
    "    progress_bar.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 15\n",
    "\n",
    "def detect_bg_perpixelmodel_loop(i):\n",
    "    \"\"\"\n",
    "    Compute the background mask using the mahalanobis distance.\n",
    "    \"\"\"\n",
    "    bgmask = np.ones(shape=(testheight, testwidth, 1), dtype=np.int16)\n",
    "    \n",
    "    for h in range(testheight):\n",
    "        for w in range(testwidth):\n",
    "            residual = testvid[i, h, w]-means[h, w]\n",
    "            if residual.T @ inv_cov_matrices[h, w] @ residual < THRESHOLD**2:\n",
    "                bgmask[h, w] = 0\n",
    "\n",
    "    return bgmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = widgets.Play(\n",
    "    interval=3000,\n",
    "    min=0,\n",
    "    max=testnumframes-1,\n",
    "    step=1,\n",
    ")\n",
    "\n",
    "def print_frame(i):\n",
    "    imshow(testvid[i]*detect_bg_perpixelmodel_loop(i))\n",
    "    \n",
    "slider = interactive(print_frame, i=(0, testnumframes-1, 1))\n",
    "widgets.jslink((play, 'value'), (slider.children[0], 'value'))\n",
    "widgets.VBox([play, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAMPLE EXAM QUESTION\n",
    "\n",
    "![alt text](F1.png \"Figure 1\")\n",
    "\n",
    "![alt text](Q1.png \"Question 1\")\n",
    "\n",
    "![alt text](Q2.png \"Question 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAMPLE EXAM SOLUTION\n",
    "\n",
    "#### Answer 1:\n",
    "We can achieve the required segmentation by using simple thresholding on the pixel brightness. In the histogram there is a large region of middle brightness corresponding to the background, a smaller region of high brightness corresponding to the coins, and a very small dark tail which corresponds to the shadows. Not only the brightness values, but also the area subsumed to the histogram regions helps to identify the extension of corresponding regions. Of course we need regions to be clearly distinct in brightness, which is the case in this image.\n",
    "\n",
    "Thresholds can be roughly set by looking at the histogram: the coins region should be considered as the pixels with value V > Tc with Tc having some value between 150 and 200, take 170. For the shadow we can follow the same reasoning and segment for the pixel values V < Ts with Ts taking some value at the beginning of the tail, take\n",
    "75.\n",
    "\n",
    "#### Answer 2:\n",
    "We can extract connected regions. Each separate region is an individual coin instance.\n",
    "(1 pt)\n",
    "\n",
    "The possible downsides are that unperfect segmentation can easily display artifacts\n",
    "from the background that count as connected regions and need to be filtered out (by\n",
    "region area, for example), and poorly illuminated coins can result in multiple separate\n",
    "smaller regions, even of comparable size. Also if coins are overlapping, they are going\n",
    "to be counted as a single coin. (1 pt)\n",
    "\n",
    "#### Answer 2 (alternative)\n",
    "We can fit circles on the mask, for example by Hough transform (covered later in this course!). Each circle hopefully corresponds to a coin. (1 pt)\n",
    "\n",
    "The possible downsides are that unperfect segmentation can easily display artifacts\n",
    "from the background that can be fit to smaller circles and need to be\n",
    "filtered out (by radius, for example), and poorly illuminated coins can result in noisy\n",
    "patterns that could be fit to multiple smaller circles rather than the big one. If coins\n",
    "are overlapping, instead, this should work relatively well. (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Exam tips\n",
    "\n",
    "#### Exam structure\n",
    "Normally, for Visual Computing we prepare a written exam with 180 points in total, and give you 180 minutes to complete. Every TA who made a tutorial one week will prepare questions for you for approximately 18 points on the topic of the tutorial (but questions always include both topics of tutorials and of the main lectures). This means that topics are represented quite uniformly in the exam, at the granularity of the lecture weeks.\n",
    "\n",
    "Regarding materials, you are allowed to bring 4 A4-sized, double-sided, handwritten pieces of paper (no print, not even from tablet handwriting) and a non programmable calculator.\n",
    "\n",
    "Overall, it is normal to not finish the exam within the 3 hours allowed. Indeed, we don't expect you to score 180 points. Every year we adjust the grading scale to better match the difficulty of the exam we made. For example, in 2021 you needed 90 points for a 4.0, and 145 points for a 6.0.\n",
    "\n",
    "#### How to prepare\n",
    "1. Since the exam is under time pressure, we strongly suggest to ignore the order the questions are presented to you. Instead, since there is plenty of questions to pool from, we suggest to prioritize the topics you feel confident with. \n",
    "\n",
    "2. If you can answer a question only partially, keep in mind that we do award partial scores. However, don't waste your time by guessing, or going clearly out of topic. Time is the real scarse resource on this exam.\n",
    "\n",
    "3. Try to be uniformly prepared on all the topics. If you only master a single topic, the most you can get is 18 points. We think the best strategy for preparation is to improve on your weakest topic.\n",
    "\n",
    "4. Don't over-optimize for the exam. Getting a good grade is good, but enjoying the course is better. We won't make any coding questions, and we will not grade these tutorials, but we believe that these will allow you to learn more effectively by doing, rather than just plain studying. Take the chance!\n",
    "\n",
    "\n",
    "I hope these tips are going to be helpful! We wish you a successful journey into the Visual Computing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
